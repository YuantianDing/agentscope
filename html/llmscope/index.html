<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>llmscope API documentation</title>
<meta name="description" content="`llmscope` …">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>llmscope</code></h1>
</header>
<section id="section-intro">
<h1 id="llmscope"><code><a title="llmscope" href="#llmscope">llmscope</a></code></h1>
<p><code><a title="llmscope" href="#llmscope">llmscope</a></code> is a Python library designed to simplify interactions with Large Language Models (LLMs) by providing a stateful, fluent interface for managing conversation history, tool usage, and response parsing. It leverages libraries like <code>mirascope</code> for LLM calls and <code>pydantic</code> for data validation and parsing.</p>
<h2 id="a-taste-of-llmscope">A Taste of <code><a title="llmscope" href="#llmscope">llmscope</a></code></h2>
<pre><code class="language-python">from pydantic import BaseModel
import llmscope

class CodeItemDoc(BaseModel):
    summary: str = Field(&quot;A concise description of this function/struct/enum... &quot;)
    example: str = Field(&quot;Provide an example of how this item is used. &quot;)

@llmscope.fn(&quot;openai&quot;, model=&quot;gpt-4o&quot;)
def generate_doc(llm, code: str, item_type: Literal[&quot;function&quot;, &quot;struct&quot;]) -&gt; CodeItemDoc:
    # Organize your prompts like `print`
    llm.system(&quot;You are a professional software engineer writing documentation. &quot;)
    if item_type == &quot;function&quot;:
        llm.system(&quot;* For functions, start with a verb and describe the functionality.&quot;)
    else:
        llm.system(&quot;* For structs, start with a noun phrase summarizing this type.&quot;)

    llm.user(code)

    # Elegant structured output selectors.
    # Using `os.fork` to collect different json schemas in different places.
    # Equilvalent to mirascope.llm.call(tools=[ViewCodeSpace], response_model=CodeItemDoc).
    while tool := llm.try_tool(ViewCodeSpace):
        llm.assistant(tool.call())

    return llm.parse(CodeItemDoc) 
</code></pre>
<h2 id="installation">Installation</h2>
<pre><code class="language-bash">pip install llmscope[openai,anthropic,...] 
</code></pre>
<p><em>(Note: Similar to Mirascope, <code><a title="llmscope" href="#llmscope">llmscope</a></code> also relies upon different dependencies for different LLM providers, see full list of providers: <a href="https://mirascope.com/api/llm/call/">https://mirascope.com/api/llm/call/</a>)</em></p>
<h2 id="usage">Usage</h2>
<p>Use the <code>@fn</code> decorator to wrap a function that defines the agent's behavior. This decorator automatically injects an <code><a title="llmscope.LLMState" href="#llmscope.LLMState">LLMState</a></code> instance.</p>
<pre><code class="language-python">from llmscope import fn, BaseTool, Field
# Assuming necessary imports for Provider, BaseMessageParam, etc.

# Define a tool (using mirascope's BaseTool)
class EmotionTool(BaseTool):
    &quot;&quot;&quot;Tool to represent a chosen emotion.&quot;&quot;&quot;
    emotion: str = Field(..., description=&quot;The name of the emotion.&quot;)
    reason: str = Field(..., description=&quot;A brief reason for choosing this emotion.&quot;)

    def call(self):
        print(f&quot;Tool Call: Emotion={self.emotion}, Reason={self.reason}&quot;)
        return f&quot;Emotion {self.emotion} acknowledged.&quot;

# Define the agent function
@fn(provider=&quot;openai&quot;, model=&quot;gpt-4o&quot;) # Configure provider and model
def emotion_agent(llm, initial_prompt: str):
    llm.system(&quot;You are an llm that chooses emotions when asked.&quot;)
    llm.user(initial_prompt)

    # Loop while the LLM decides to use the EmotionTool
    while tool_call := llm.try_tool(EmotionTool):
        result = tool_call.call() # Execute the tool
        print(f&quot;Tool Result: {result}&quot;)
        # Add tool execution result back to the conversation
        llm.assistant(f&quot;Okay, I chose {tool_call.emotion}.&quot;) # Or use llm.tool(tool_call=..., content=result) with mirascope
        llm.user(&quot;Okay, choose another different emotion and explain why.&quot;)

    # If no tool is called, get the final text response
    try:
        final_response = llm.generate()
        print(f&quot;Agent's final text response: {final_response.content}&quot;)
    except Exception as e:
        # Handle cases where generate might fail or is used incorrectly (e.g., after try_parse)
        print(f&quot;Could not generate final response: {e}&quot;)

    return &quot;Agent finished.&quot;

# Run the llm
result = emotion_agent(&quot;Choose an emotion and explain why.&quot;)
print(result)
</code></pre>
<h3 id="2-key-llmstate-methods">2. Key <code><a title="llmscope.LLMState" href="#llmscope.LLMState">LLMState</a></code> Methods</h3>
<ul>
<li><code>config(provider=..., model=..., call_params=...)</code>: Sets the LLM provider, model, and optional call parameters.</li>
<li><code>msg(role, *message)</code>: Adds a message to the history.</li>
<li><code>system(*message)</code>, <code>user(*message)</code>, <code>assistant(*message)</code>: Convenience methods for <code>msg</code>.</li>
<li><code>try_tool(ToolClass)</code>: Attempts to get the LLM to use the specified tool. Returns the tool instance if successful, <code>None</code> otherwise. Can be used in a loop.</li>
<li><code>try_tools(*ToolClasses)</code>: Similar to <code>try_tool</code> but for multiple possible tools. Returns a list of successful tool calls.</li>
<li><code>try_parse(PydanticModel)</code>: Attempts to parse the LLM response into the given Pydantic model <em>without</em> finalizing the request. Useful for checking intermediate structured outputs.</li>
<li><code>parse(PydanticModel)</code>: <strong>Finalizes</strong> the request and parses the LLM response into the given Pydantic model. Raises <code>ValidationError</code> on failure.</li>
<li><code>generate()</code>: <strong>Finalizes</strong> the request and returns the raw LLM response (<code>BaseCallResponse</code> from mirascope), typically used when no specific parsing or tool use is expected at the end.</li>
</ul>
<p><strong>Important:</strong> Methods like <code>try_tool</code>, <code>try_parse</code>, <code>parse</code>, and <code>generate</code> trigger internal state management and potentially LLM calls. Avoid calling <code>config</code> or <code>msg</code> <em>between</em> a <code>try_</code> call and its corresponding <code>parse</code> or <code>generate</code> call within the same logical block.</p>
<h2 id="license">License</h2>
<p><a href="https://mit-license.org/">MIT</a></p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="llmscope.fn"><code class="name flex">
<span>def <span class="ident">fn</span></span>(<span>provider: Literal['anthropic', 'azure', 'bedrock', 'cohere', 'gemini', 'google', 'groq', 'litellm', 'mistral', 'openai', 'vertex', 'xai'] | None = None,<br>model: str | None = None) ‑> <built-in function callable></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fn(provider: Provider | None = None, model: str | None = None) -&gt; callable:
    &#34;&#34;&#34;Create a decorator to initialize and inject LLMState into a function.

    This function acts as a factory that generates a decorator. When this
    decorator is applied to a function, it modifies the function&#39;s behavior.
    The decorated function will receive an `LLMState` object as its first
    argument. This `LLMState` object can be used for LLM generation,
    managing messages, and handling tools and response schemas.

    The decorated function is expected to accept `LLMState` as its first
    positional argument, followed by its original arguments (`*args`, `**kwargs`),
    and should return a `runner` object.

    Args:
        provider (Provider | None, optional): The provider instance to assign
            to `state.provider`. Defaults to None.
        model (str | None, optional): The model identifier string to assign
            to `state.model`. Defaults to None.

    Returns:
        callable: A decorator function that wraps the target function,
                  injects the configured `LLMState`, and returns the
                  `runner` produced by the target function.
    &#34;&#34;&#34;
    def decorator(func):
        def wrapper(*args, **kwargs):
            state = LLMState()
            state.provider = provider
            state.model = model
            runner = func(state, *args, **kwargs)
            return runner
        return wrapper
    return decorator</code></pre>
</details>
<div class="desc"><p>Create a decorator to initialize and inject LLMState into a function.</p>
<p>This function acts as a factory that generates a decorator. When this
decorator is applied to a function, it modifies the function's behavior.
The decorated function will receive an <code><a title="llmscope.LLMState" href="#llmscope.LLMState">LLMState</a></code> object as its first
argument. This <code><a title="llmscope.LLMState" href="#llmscope.LLMState">LLMState</a></code> object can be used for LLM generation,
managing messages, and handling tools and response schemas.</p>
<p>The decorated function is expected to accept <code><a title="llmscope.LLMState" href="#llmscope.LLMState">LLMState</a></code> as its first
positional argument, followed by its original arguments (<code>*args</code>, <code>**kwargs</code>),
and should return a <code>runner</code> object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>provider</code></strong> :&ensp;<code>Provider | None</code>, optional</dt>
<dd>The provider instance to assign
to <code>state.provider</code>. Defaults to None.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>str | None</code>, optional</dt>
<dd>The model identifier string to assign
to <code>state.model</code>. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>callable</code></dt>
<dd>A decorator function that wraps the target function,
injects the configured <code><a title="llmscope.LLMState" href="#llmscope.LLMState">LLMState</a></code>, and returns the
<code>runner</code> produced by the target function.</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="llmscope.LLMRequest"><code class="flex name class">
<span>class <span class="ident">LLMRequest</span></span>
<span>(</span><span>provider: Literal['anthropic', 'azure', 'bedrock', 'cohere', 'gemini', 'google', 'groq', 'litellm', 'mistral', 'openai', 'vertex', 'xai'],<br>model: str,<br>messages: list[mirascope.core.base.message_param.BaseMessageParam],<br>tools: list[mirascope.core.base.tool.BaseTool],<br>call_params: dict | None = None,<br>response_model: pydantic.main.BaseModel | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@dataclass
class LLMRequest(BaseException):
    &#34;&#34;&#34;Represents a request to the language model.&#34;&#34;&#34;
    provider: Provider
    model: str
    messages: list[BaseMessageParam]
    tools: list[BaseTool]
    call_params: dict | None = None
    response_model: BaseModel | None = None

    def generate(self):
        &#34;&#34;&#34;Generates a response from the language model based on the request.&#34;&#34;&#34;
        kwargs = {}
        if self.call_params is not None:
            kwargs[&#34;call_params&#34;] = self.call_params
        if self.response_model is not None:
            kwargs[&#34;response_model&#34;] = self.response_model
        return llm.call(provider=self.provider, model=self.model, tools=self.tools, **kwargs)(lambda: self.messages)()</code></pre>
</details>
<div class="desc"><p>Represents a request to the language model.</p></div>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.BaseException</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="llmscope.LLMRequest.call_params"><code class="name">var <span class="ident">call_params</span> : dict | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="llmscope.LLMRequest.messages"><code class="name">var <span class="ident">messages</span> : list[mirascope.core.base.message_param.BaseMessageParam]</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="llmscope.LLMRequest.model"><code class="name">var <span class="ident">model</span> : str</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="llmscope.LLMRequest.provider"><code class="name">var <span class="ident">provider</span> : Literal['anthropic', 'azure', 'bedrock', 'cohere', 'gemini', 'google', 'groq', 'litellm', 'mistral', 'openai', 'vertex', 'xai']</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="llmscope.LLMRequest.response_model"><code class="name">var <span class="ident">response_model</span> : pydantic.main.BaseModel | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="llmscope.LLMRequest.tools"><code class="name">var <span class="ident">tools</span> : list[mirascope.core.base.tool.BaseTool]</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="llmscope.LLMRequest.generate"><code class="name flex">
<span>def <span class="ident">generate</span></span>(<span>self)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate(self):
    &#34;&#34;&#34;Generates a response from the language model based on the request.&#34;&#34;&#34;
    kwargs = {}
    if self.call_params is not None:
        kwargs[&#34;call_params&#34;] = self.call_params
    if self.response_model is not None:
        kwargs[&#34;response_model&#34;] = self.response_model
    return llm.call(provider=self.provider, model=self.model, tools=self.tools, **kwargs)(lambda: self.messages)()</code></pre>
</details>
<div class="desc"><p>Generates a response from the language model based on the request.</p></div>
</dd>
</dl>
</dd>
<dt id="llmscope.LLMState"><code class="flex name class">
<span>class <span class="ident">LLMState</span></span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LLMState:
    &#34;&#34;&#34;
    Manages the state and configuration for an llm&#39;s interaction with a language model.
    This class provides a fluent interface for building LLM requests, including setting
    the provider and model, adding messages, defining tools, and specifying response
    parsing models. It uses an internal mechanism involving subprocesses (`_command`)
    to collect tool and response schema definitions across chained calls (`try_tools`,
    `try_parse`) before finally executing the LLM request (`generate`, `parse`).
    Attributes:
        provider (Provider | None): The LLM provider instance (e.g., OpenAI, Anthropic).
        model (str | None): The specific model name to use (e.g., &#34;gpt-4o&#34;).
        messages (list[BaseMessageParam]): A list of messages constituting the
            conversation history.
    &#34;&#34;&#34;
    provider: Provider | None = None
    model: str | None = None
    messages: list[BaseMessageParam] = []
    call_params: dict | None = None
    _tools: list[BaseTool] = []
    _response_model: list[type] = []
    _response_value: Any = None
    _child_writeback: Any = None

    def __init__(self):
        pass
    
    def config(self, provider: Provider | None = None, model: str | None = None, call_params: dict | None = None):
        &#34;&#34;&#34;Configure the llm&#39;s provider, model, and call parameters.

        This method updates the llm&#39;s configuration. It includes an assertion
        to prevent configuration changes between specific method calls like
        `try_tools`, `try_tool`, `try_parse`, `parse`, and `generate`.

        Args:
            provider (Provider | None, optional): The API provider to set for the llm.
                If None, the current provider remains unchanged. Defaults to None.
            model (str | None, optional): The model name to set for the llm.
                If None, the current model remains unchanged. Defaults to None.
            call_params (dict | None, optional): The call parameters to set for the llm&#39;s
                API calls. If None, the current call parameters remain unchanged.
                Defaults to None.

        Returns:
            self: The llm instance itself, allowing for method chaining.

        Raises:
            AssertionError: If called between `try_tools`, `try_tool`, `try_parse`,
                `parse` and `generate` methods.
        &#34;&#34;&#34;
        assert self._child_writeback is None and self._response_value is None, &#34;Called `llm.config` between `try_tools`, `try_tool`, `try_parse`, `parse` and `generate`.&#34;
        if provider is not None:
            self.provider = provider
        if model is not None:
            self.model = model
        if call_params is not None:
            self.call_params = call_params
        return self

    def msg(self, role: str, *message: list[Any]):
        &#34;&#34;&#34;Append a message to the llm&#39;s memory.

        This method takes a role and one or more message strings, processes
        them, and adds them to the llm&#39;s message history (`self.messages`).
        If the last message in the history has the same role, the new
        content is appended to it. Otherwise, a new message object is created.
        It also use `textwrap.dedent` to remove common leading whitespace from the
        message content.

        Args:
            role (str): The role of the message sender (e.g., &#34;user&#34;,
                &#34;assistant&#34;, &#34;system&#34;).
            *message (list[Any]): One or more message parts (typically strings)
                to be combined into a single message content. Each part is
                dedented before joining.

        Returns:
            LLMState: The llm instance itself, allowing for method chaining.

        Raises:
            AssertionError: If called between specific asynchronous operations
                like `try_tools`, `try_tool`, `try_parse`, `parse`, or
                `generate`, indicating improper usage.
        &#34;&#34;&#34;
        message = &#34; &#34;.join(textwrap.dedent(m) for m in message) + &#34;\n&#34;
        assert self._child_writeback is None and self._response_value is None, &#34;Called `llm.msg` between `try_tools`, `try_tool`, `try_parse`, `parse` and `generate`.&#34;
        if len(self.messages) &gt; 0 and self.messages[-1].role == role:
            self.messages[-1].content += &#34; &#34;.join(message) + &#34;\n&#34;
        else:
            self.messages.append(BaseMessageParam(role=role, content=&#34; &#34;.join(message) + &#34;\n&#34;))
        return self
    
    def system(self, *message: list[Any]):
        &#34;&#34;&#34;Send a system message. See `msg` for more details.

        Args:
            *message: A list of messages to be sent as system messages.

        Returns:
            LLMState: The llm instance itself, allowing for method chaining.
        &#34;&#34;&#34;
        return self.msg(&#34;system&#34;, *message)
        
    def user(self, *message: list[Any]):
        &#34;&#34;&#34;Send a user message. See `msg` for more details.

        Args:
            *message: A list of messages to be sent as system messages.

        Returns:
            LLMState: The llm instance itself, allowing for method chaining.
        &#34;&#34;&#34;
        return self.msg(&#34;user&#34;, *message)
    
    def assistant(self, *message: list[Any]):
        &#34;&#34;&#34;Send a assistant message. See `msg` for more details.

        Args:
            *message: A list of messages to be sent as system messages.

        Returns:
            LLMState: The llm instance itself, allowing for method chaining.
        &#34;&#34;&#34;
        return self.msg(&#34;assistant&#34;, *message)
    
    def _command(self, update_state: callable, validate: callable, final: bool = False) -&gt; Any | None:
        # If it is subprcess, update current state and return to the main process later
        if self._child_writeback is not None:
            update_state()
            if final:
                with os.fdopen(self._child_writeback, &#34;wb&#34;) as write_pipe:
                    pickled_types = pickle.dumps((self._tools, self._response_model))
                    write_pipe.write(pickled_types)
                os._exit(0)
            else:
                return None
        # If it is main process, we need to check if the response value is already exists and validate it.
        if self._response_value is not None:
            resp = validate(self._response_value)
            if resp is not None:
                self._response_value = None
                self._tools = []
                self._response_model = []
                return resp
            else:
                if final:
                    raise ValidationError(f&#34;Invalid response value: {type(self._response_value)}. Expected: {self._response_model + self._tools} &#34;)
                return None
        # Otherwise 1. Create a subprocess to collect the _tools and response schema, 2. Generate a value and check.
        else:
            if final:
                assert self._tools == [], &#34;Unknown Error&#34;
                assert self._response_model == [], &#34;Unknown Error&#34;
                update_state()
            else:
                r, w = os.pipe()
                pid = os.fork()
                if pid == 0:
                    os.close(r)
                    self._child_writeback = w
                    return self._command(update_state, validate, final)
                os.close(w)
                os.waitpid(pid, 0)
                with os.fdopen(r, &#34;rb&#34;) as read_pipe:
                    pickled_types = read_pipe.read()
                (self._tools, self._response_model) = pickle.loads(pickled_types)
            assert all(issubclass(m, BaseTool) for m in self._tools)
            assert all(issubclass(m, BaseModel) for m in self._response_model)
            self._work_value()
            return self._command(update_state, validate, final)
    
    def try_tools(self, *tools: list[type]) -&gt; list[BaseTool] | None:
        &#34;&#34;&#34;Attempts to let the llm use a specific set of tools.

        This method uses `os.fork` to create a subprocess that collects the
        tools and response schema and assemble all these type information into a
        request to the language model. 
        Args:
            *tools (list[type]): A variable number of tool types (classes) to
                try. Each type must be a subclass of `BaseTool`.
        Returns:
            list[BaseTool] | None: A list containing the validated tool call
                objects from the response if the validation is successful for
                at least one of the provided tools. Returns `None` if the
                response does not contain a valid call to any of the specified
                tools.
        &#34;&#34;&#34;
        assert all(issubclass(t, BaseTool) for t in tools), &#34;All tools must be subclass of `BaseTool`.&#34;
        def validate(value: BaseCallResponse):
            if not hasattr(value, &#39;tools&#39;) or type(value.tools) is not list:
                return None
            
            v = [v for v in value.tools if any(_validate(t, v.model_dump()) and v.tool_call.function.name == t.__name__ for t in tools)]
            if len(v) == 0:
                return None
            return v
        return self._command(
            update_state=lambda: self._tools.extend(tools),
            validate=validate,
        )
    
    def try_tool(self, tool: type) -&gt; BaseTool | None:
        &#34;&#34;&#34;Attempts to let the llm use a specific tool.
        This method is a convenience wrapper around `try_tools` for a single
        tool type. It calls `try_tools` with the provided tool type and
        returns the first successfully initialized tool instance if any,
        otherwise None. See `try_tools` for more details on the underlying
        mechanism and potential error handling.
        Args:
            tool (type): The class type of the tool to attempt to use.
        Returns:
            BaseTool | None: An instance of the tool if successfully
                initialized and added, otherwise `None`.
        &#34;&#34;&#34;
        result = self.try_tools(tool)
        if type(result) == list and len(result) &gt; 0:
            return result[0]
        else:
            return None
    

    def try_parse(self, ty: type, final: bool = False) -&gt; Any:
        &#34;&#34;&#34;Attempts to parse and validate a value against the specified type.

        This method uses `os.fork` to create a subprocess that collects the
        tools and response schema and assemble all these type information into a
        request to the language model. 

        Args:
            ty (type): The expected type for the value. Must be a type
                supported by Pydantic&#39;s TypeAdapter for validation.

        Returns:
            Any: The parsed and validated value conforming to the type `ty`.
                 The exact behavior on validation failure depends on the
                 implementation of the `_command` method.

        Raises:
            AssertionError: If the provided type `ty` is not validatable
                by Pydantic.
        &#34;&#34;&#34;
        assert TypeAdapter(ty), &#34;Type must be validable through pydantic. &#34;
        return self._command(
            update_state=lambda: self._response_model.append(ty),
            validate=lambda value: value if _validate(ty, value) else None,
            final=final
        )

    def parse(self, ty: type) -&gt; Any:
        &#34;&#34;&#34;Parse the content into the given type.

        Note that when this method is often used in conjunction with `try_parse` and `try_tools`.
        `try_parse` or `try_tools` uses `os.fork` to create a subprocess that collects the
        tools and response schema and assemble all these type information into a
        request to the language model. The forked process will be ended here, returning
        all type information (tools and response schemas) into the main process.

        This method can also be used alone, in which case it will not use `os.fork` to create a subprocess.
        It will directly generate the response and parse it into the given type.

        Args:
            ty (type): The target type to parse the content into.

        Returns:
            Any: An instance of the specified type `ty` representing the
                 parsed content.

        Raises:
            ValueError: If the content cannot be successfully parsed into the
                        specified type `ty`.
        &#34;&#34;&#34;
        return self.try_parse(ty, final=True)
    
    def generate(self) -&gt; BaseCallResponse:
        &#34;&#34;&#34;Generate a response from the language model.

        Note that when this method is often used in conjunction with `try_tools` but not `try_parse`.
        `try_tools` uses `os.fork` to create a subprocess that collects the
        tools and response schema and assemble all these type information into a
        request to the language model. The forked process will be ended here, returning
        all type information (tools and response schemas) into the main process.

        This method can also be used alone, in which case it will not use `os.fork` to create a subprocess.
        It will directly generate the response and parse it into the given type.

        The method will return AssertionError when used with `try_parse`.


        Returns:
            BaseCallResponse: The generated response from the language model, from `mirascope`.

        Raises:
            AssertionError: when used with `try_parse`.
        &#34;&#34;&#34;
        assert len(self._response_model) == 0, &#34;Must not use `generate` together with `try_parse`.&#34;
        return self._command(
            update_state=lambda: None,
            validate=lambda value: BaseCallResponse.model_validate(value),
            final=True
        )

    def _work_value(self):
        assert self.provider is not None, &#34;Provider must be set. See https://mirascope.com/api/llm/call/.&#34;
        assert self.model is not None, &#34;Model must be set. See https://mirascope.com/api/llm/call/. &#34;
        if len(self._response_model) == 0:
            response_model = None
        elif len(self._response_model) == 1:
            response_model = self._response_model[0]
        else:
            response_model = Union[*self._response_model]

        response = LLMRequest(
            provider=self.provider,
            model=self.model,
            messages=self.messages,
            tools=self._tools,
            call_params=self.call_params,
            response_model=response_model,
        ).generate()
        if response is None:
            raise RuntimeError(&#34;No response from LLM.&#34;)
        self._response_value = response
        self._tools = []
        self._response_model = []</code></pre>
</details>
<div class="desc"><p>Manages the state and configuration for an llm's interaction with a language model.
This class provides a fluent interface for building LLM requests, including setting
the provider and model, adding messages, defining tools, and specifying response
parsing models. It uses an internal mechanism involving subprocesses (<code>_command</code>)
to collect tool and response schema definitions across chained calls (<code>try_tools</code>,
<code>try_parse</code>) before finally executing the LLM request (<code>generate</code>, <code>parse</code>).</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>provider</code></strong> :&ensp;<code>Provider | None</code></dt>
<dd>The LLM provider instance (e.g., OpenAI, Anthropic).</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>str | None</code></dt>
<dd>The specific model name to use (e.g., "gpt-4o").</dd>
<dt><strong><code>messages</code></strong> :&ensp;<code>list[BaseMessageParam]</code></dt>
<dd>A list of messages constituting the
conversation history.</dd>
</dl></div>
<h3>Class variables</h3>
<dl>
<dt id="llmscope.LLMState.call_params"><code class="name">var <span class="ident">call_params</span> : dict | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="llmscope.LLMState.messages"><code class="name">var <span class="ident">messages</span> : list[mirascope.core.base.message_param.BaseMessageParam]</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="llmscope.LLMState.model"><code class="name">var <span class="ident">model</span> : str | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
<dt id="llmscope.LLMState.provider"><code class="name">var <span class="ident">provider</span> : Literal['anthropic', 'azure', 'bedrock', 'cohere', 'gemini', 'google', 'groq', 'litellm', 'mistral', 'openai', 'vertex', 'xai'] | None</code></dt>
<dd>
<div class="desc"><p>The type of the None singleton.</p></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="llmscope.LLMState.assistant"><code class="name flex">
<span>def <span class="ident">assistant</span></span>(<span>self, *message: list[typing.Any])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def assistant(self, *message: list[Any]):
    &#34;&#34;&#34;Send a assistant message. See `msg` for more details.

    Args:
        *message: A list of messages to be sent as system messages.

    Returns:
        LLMState: The llm instance itself, allowing for method chaining.
    &#34;&#34;&#34;
    return self.msg(&#34;assistant&#34;, *message)</code></pre>
</details>
<div class="desc"><p>Send a assistant message. See <code>msg</code> for more details.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*message</code></strong></dt>
<dd>A list of messages to be sent as system messages.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="llmscope.LLMState" href="#llmscope.LLMState">LLMState</a></code></dt>
<dd>The llm instance itself, allowing for method chaining.</dd>
</dl></div>
</dd>
<dt id="llmscope.LLMState.config"><code class="name flex">
<span>def <span class="ident">config</span></span>(<span>self,<br>provider: Literal['anthropic', 'azure', 'bedrock', 'cohere', 'gemini', 'google', 'groq', 'litellm', 'mistral', 'openai', 'vertex', 'xai'] | None = None,<br>model: str | None = None,<br>call_params: dict | None = None)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def config(self, provider: Provider | None = None, model: str | None = None, call_params: dict | None = None):
    &#34;&#34;&#34;Configure the llm&#39;s provider, model, and call parameters.

    This method updates the llm&#39;s configuration. It includes an assertion
    to prevent configuration changes between specific method calls like
    `try_tools`, `try_tool`, `try_parse`, `parse`, and `generate`.

    Args:
        provider (Provider | None, optional): The API provider to set for the llm.
            If None, the current provider remains unchanged. Defaults to None.
        model (str | None, optional): The model name to set for the llm.
            If None, the current model remains unchanged. Defaults to None.
        call_params (dict | None, optional): The call parameters to set for the llm&#39;s
            API calls. If None, the current call parameters remain unchanged.
            Defaults to None.

    Returns:
        self: The llm instance itself, allowing for method chaining.

    Raises:
        AssertionError: If called between `try_tools`, `try_tool`, `try_parse`,
            `parse` and `generate` methods.
    &#34;&#34;&#34;
    assert self._child_writeback is None and self._response_value is None, &#34;Called `llm.config` between `try_tools`, `try_tool`, `try_parse`, `parse` and `generate`.&#34;
    if provider is not None:
        self.provider = provider
    if model is not None:
        self.model = model
    if call_params is not None:
        self.call_params = call_params
    return self</code></pre>
</details>
<div class="desc"><p>Configure the llm's provider, model, and call parameters.</p>
<p>This method updates the llm's configuration. It includes an assertion
to prevent configuration changes between specific method calls like
<code>try_tools</code>, <code>try_tool</code>, <code>try_parse</code>, <code>parse</code>, and <code>generate</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>provider</code></strong> :&ensp;<code>Provider | None</code>, optional</dt>
<dd>The API provider to set for the llm.
If None, the current provider remains unchanged. Defaults to None.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>str | None</code>, optional</dt>
<dd>The model name to set for the llm.
If None, the current model remains unchanged. Defaults to None.</dd>
<dt><strong><code>call_params</code></strong> :&ensp;<code>dict | None</code>, optional</dt>
<dd>The call parameters to set for the llm's
API calls. If None, the current call parameters remain unchanged.
Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>self</code></dt>
<dd>The llm instance itself, allowing for method chaining.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>AssertionError</code></dt>
<dd>If called between <code>try_tools</code>, <code>try_tool</code>, <code>try_parse</code>,
<code>parse</code> and <code>generate</code> methods.</dd>
</dl></div>
</dd>
<dt id="llmscope.LLMState.generate"><code class="name flex">
<span>def <span class="ident">generate</span></span>(<span>self) ‑> mirascope.core.base.call_response.BaseCallResponse</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate(self) -&gt; BaseCallResponse:
    &#34;&#34;&#34;Generate a response from the language model.

    Note that when this method is often used in conjunction with `try_tools` but not `try_parse`.
    `try_tools` uses `os.fork` to create a subprocess that collects the
    tools and response schema and assemble all these type information into a
    request to the language model. The forked process will be ended here, returning
    all type information (tools and response schemas) into the main process.

    This method can also be used alone, in which case it will not use `os.fork` to create a subprocess.
    It will directly generate the response and parse it into the given type.

    The method will return AssertionError when used with `try_parse`.


    Returns:
        BaseCallResponse: The generated response from the language model, from `mirascope`.

    Raises:
        AssertionError: when used with `try_parse`.
    &#34;&#34;&#34;
    assert len(self._response_model) == 0, &#34;Must not use `generate` together with `try_parse`.&#34;
    return self._command(
        update_state=lambda: None,
        validate=lambda value: BaseCallResponse.model_validate(value),
        final=True
    )</code></pre>
</details>
<div class="desc"><p>Generate a response from the language model.</p>
<p>Note that when this method is often used in conjunction with <code>try_tools</code> but not <code>try_parse</code>.
<code>try_tools</code> uses <code>os.fork</code> to create a subprocess that collects the
tools and response schema and assemble all these type information into a
request to the language model. The forked process will be ended here, returning
all type information (tools and response schemas) into the main process.</p>
<p>This method can also be used alone, in which case it will not use <code>os.fork</code> to create a subprocess.
It will directly generate the response and parse it into the given type.</p>
<p>The method will return AssertionError when used with <code>try_parse</code>.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BaseCallResponse</code></dt>
<dd>The generated response from the language model, from <code>mirascope</code>.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>AssertionError</code></dt>
<dd>when used with <code>try_parse</code>.</dd>
</dl></div>
</dd>
<dt id="llmscope.LLMState.msg"><code class="name flex">
<span>def <span class="ident">msg</span></span>(<span>self, role: str, *message: list[typing.Any])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def msg(self, role: str, *message: list[Any]):
    &#34;&#34;&#34;Append a message to the llm&#39;s memory.

    This method takes a role and one or more message strings, processes
    them, and adds them to the llm&#39;s message history (`self.messages`).
    If the last message in the history has the same role, the new
    content is appended to it. Otherwise, a new message object is created.
    It also use `textwrap.dedent` to remove common leading whitespace from the
    message content.

    Args:
        role (str): The role of the message sender (e.g., &#34;user&#34;,
            &#34;assistant&#34;, &#34;system&#34;).
        *message (list[Any]): One or more message parts (typically strings)
            to be combined into a single message content. Each part is
            dedented before joining.

    Returns:
        LLMState: The llm instance itself, allowing for method chaining.

    Raises:
        AssertionError: If called between specific asynchronous operations
            like `try_tools`, `try_tool`, `try_parse`, `parse`, or
            `generate`, indicating improper usage.
    &#34;&#34;&#34;
    message = &#34; &#34;.join(textwrap.dedent(m) for m in message) + &#34;\n&#34;
    assert self._child_writeback is None and self._response_value is None, &#34;Called `llm.msg` between `try_tools`, `try_tool`, `try_parse`, `parse` and `generate`.&#34;
    if len(self.messages) &gt; 0 and self.messages[-1].role == role:
        self.messages[-1].content += &#34; &#34;.join(message) + &#34;\n&#34;
    else:
        self.messages.append(BaseMessageParam(role=role, content=&#34; &#34;.join(message) + &#34;\n&#34;))
    return self</code></pre>
</details>
<div class="desc"><p>Append a message to the llm's memory.</p>
<p>This method takes a role and one or more message strings, processes
them, and adds them to the llm's message history (<code>self.messages</code>).
If the last message in the history has the same role, the new
content is appended to it. Otherwise, a new message object is created.
It also use <code>textwrap.dedent</code> to remove common leading whitespace from the
message content.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>role</code></strong> :&ensp;<code>str</code></dt>
<dd>The role of the message sender (e.g., "user",
"assistant", "system").</dd>
<dt><strong><code>*message</code></strong> :&ensp;<code>list[Any]</code></dt>
<dd>One or more message parts (typically strings)
to be combined into a single message content. Each part is
dedented before joining.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="llmscope.LLMState" href="#llmscope.LLMState">LLMState</a></code></dt>
<dd>The llm instance itself, allowing for method chaining.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>AssertionError</code></dt>
<dd>If called between specific asynchronous operations
like <code>try_tools</code>, <code>try_tool</code>, <code>try_parse</code>, <code>parse</code>, or
<code>generate</code>, indicating improper usage.</dd>
</dl></div>
</dd>
<dt id="llmscope.LLMState.parse"><code class="name flex">
<span>def <span class="ident">parse</span></span>(<span>self, ty: type) ‑> Any</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse(self, ty: type) -&gt; Any:
    &#34;&#34;&#34;Parse the content into the given type.

    Note that when this method is often used in conjunction with `try_parse` and `try_tools`.
    `try_parse` or `try_tools` uses `os.fork` to create a subprocess that collects the
    tools and response schema and assemble all these type information into a
    request to the language model. The forked process will be ended here, returning
    all type information (tools and response schemas) into the main process.

    This method can also be used alone, in which case it will not use `os.fork` to create a subprocess.
    It will directly generate the response and parse it into the given type.

    Args:
        ty (type): The target type to parse the content into.

    Returns:
        Any: An instance of the specified type `ty` representing the
             parsed content.

    Raises:
        ValueError: If the content cannot be successfully parsed into the
                    specified type `ty`.
    &#34;&#34;&#34;
    return self.try_parse(ty, final=True)</code></pre>
</details>
<div class="desc"><p>Parse the content into the given type.</p>
<p>Note that when this method is often used in conjunction with <code>try_parse</code> and <code>try_tools</code>.
<code>try_parse</code> or <code>try_tools</code> uses <code>os.fork</code> to create a subprocess that collects the
tools and response schema and assemble all these type information into a
request to the language model. The forked process will be ended here, returning
all type information (tools and response schemas) into the main process.</p>
<p>This method can also be used alone, in which case it will not use <code>os.fork</code> to create a subprocess.
It will directly generate the response and parse it into the given type.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>ty</code></strong> :&ensp;<code>type</code></dt>
<dd>The target type to parse the content into.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Any</code></dt>
<dd>An instance of the specified type <code>ty</code> representing the
parsed content.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>If the content cannot be successfully parsed into the
specified type <code>ty</code>.</dd>
</dl></div>
</dd>
<dt id="llmscope.LLMState.system"><code class="name flex">
<span>def <span class="ident">system</span></span>(<span>self, *message: list[typing.Any])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def system(self, *message: list[Any]):
    &#34;&#34;&#34;Send a system message. See `msg` for more details.

    Args:
        *message: A list of messages to be sent as system messages.

    Returns:
        LLMState: The llm instance itself, allowing for method chaining.
    &#34;&#34;&#34;
    return self.msg(&#34;system&#34;, *message)</code></pre>
</details>
<div class="desc"><p>Send a system message. See <code>msg</code> for more details.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*message</code></strong></dt>
<dd>A list of messages to be sent as system messages.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="llmscope.LLMState" href="#llmscope.LLMState">LLMState</a></code></dt>
<dd>The llm instance itself, allowing for method chaining.</dd>
</dl></div>
</dd>
<dt id="llmscope.LLMState.try_parse"><code class="name flex">
<span>def <span class="ident">try_parse</span></span>(<span>self, ty: type, final: bool = False) ‑> Any</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def try_parse(self, ty: type, final: bool = False) -&gt; Any:
    &#34;&#34;&#34;Attempts to parse and validate a value against the specified type.

    This method uses `os.fork` to create a subprocess that collects the
    tools and response schema and assemble all these type information into a
    request to the language model. 

    Args:
        ty (type): The expected type for the value. Must be a type
            supported by Pydantic&#39;s TypeAdapter for validation.

    Returns:
        Any: The parsed and validated value conforming to the type `ty`.
             The exact behavior on validation failure depends on the
             implementation of the `_command` method.

    Raises:
        AssertionError: If the provided type `ty` is not validatable
            by Pydantic.
    &#34;&#34;&#34;
    assert TypeAdapter(ty), &#34;Type must be validable through pydantic. &#34;
    return self._command(
        update_state=lambda: self._response_model.append(ty),
        validate=lambda value: value if _validate(ty, value) else None,
        final=final
    )</code></pre>
</details>
<div class="desc"><p>Attempts to parse and validate a value against the specified type.</p>
<p>This method uses <code>os.fork</code> to create a subprocess that collects the
tools and response schema and assemble all these type information into a
request to the language model. </p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>ty</code></strong> :&ensp;<code>type</code></dt>
<dd>The expected type for the value. Must be a type
supported by Pydantic's TypeAdapter for validation.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Any</code></dt>
<dd>The parsed and validated value conforming to the type <code>ty</code>.
The exact behavior on validation failure depends on the
implementation of the <code>_command</code> method.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>AssertionError</code></dt>
<dd>If the provided type <code>ty</code> is not validatable
by Pydantic.</dd>
</dl></div>
</dd>
<dt id="llmscope.LLMState.try_tool"><code class="name flex">
<span>def <span class="ident">try_tool</span></span>(<span>self, tool: type) ‑> mirascope.core.base.tool.BaseTool | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def try_tool(self, tool: type) -&gt; BaseTool | None:
    &#34;&#34;&#34;Attempts to let the llm use a specific tool.
    This method is a convenience wrapper around `try_tools` for a single
    tool type. It calls `try_tools` with the provided tool type and
    returns the first successfully initialized tool instance if any,
    otherwise None. See `try_tools` for more details on the underlying
    mechanism and potential error handling.
    Args:
        tool (type): The class type of the tool to attempt to use.
    Returns:
        BaseTool | None: An instance of the tool if successfully
            initialized and added, otherwise `None`.
    &#34;&#34;&#34;
    result = self.try_tools(tool)
    if type(result) == list and len(result) &gt; 0:
        return result[0]
    else:
        return None</code></pre>
</details>
<div class="desc"><p>Attempts to let the llm use a specific tool.
This method is a convenience wrapper around <code>try_tools</code> for a single
tool type. It calls <code>try_tools</code> with the provided tool type and
returns the first successfully initialized tool instance if any,
otherwise None. See <code>try_tools</code> for more details on the underlying
mechanism and potential error handling.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>tool</code></strong> :&ensp;<code>type</code></dt>
<dd>The class type of the tool to attempt to use.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BaseTool | None</code></dt>
<dd>An instance of the tool if successfully
initialized and added, otherwise <code>None</code>.</dd>
</dl></div>
</dd>
<dt id="llmscope.LLMState.try_tools"><code class="name flex">
<span>def <span class="ident">try_tools</span></span>(<span>self, *tools: list[type]) ‑> list[mirascope.core.base.tool.BaseTool] | None</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def try_tools(self, *tools: list[type]) -&gt; list[BaseTool] | None:
    &#34;&#34;&#34;Attempts to let the llm use a specific set of tools.

    This method uses `os.fork` to create a subprocess that collects the
    tools and response schema and assemble all these type information into a
    request to the language model. 
    Args:
        *tools (list[type]): A variable number of tool types (classes) to
            try. Each type must be a subclass of `BaseTool`.
    Returns:
        list[BaseTool] | None: A list containing the validated tool call
            objects from the response if the validation is successful for
            at least one of the provided tools. Returns `None` if the
            response does not contain a valid call to any of the specified
            tools.
    &#34;&#34;&#34;
    assert all(issubclass(t, BaseTool) for t in tools), &#34;All tools must be subclass of `BaseTool`.&#34;
    def validate(value: BaseCallResponse):
        if not hasattr(value, &#39;tools&#39;) or type(value.tools) is not list:
            return None
        
        v = [v for v in value.tools if any(_validate(t, v.model_dump()) and v.tool_call.function.name == t.__name__ for t in tools)]
        if len(v) == 0:
            return None
        return v
    return self._command(
        update_state=lambda: self._tools.extend(tools),
        validate=validate,
    )</code></pre>
</details>
<div class="desc"><p>Attempts to let the llm use a specific set of tools.</p>
<p>This method uses <code>os.fork</code> to create a subprocess that collects the
tools and response schema and assemble all these type information into a
request to the language model. </p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*tools</code></strong> :&ensp;<code>list[type]</code></dt>
<dd>A variable number of tool types (classes) to
try. Each type must be a subclass of <code>BaseTool</code>.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list[BaseTool] | None</code></dt>
<dd>A list containing the validated tool call
objects from the response if the validation is successful for
at least one of the provided tools. Returns <code>None</code> if the
response does not contain a valid call to any of the specified
tools.</dd>
</dl></div>
</dd>
<dt id="llmscope.LLMState.user"><code class="name flex">
<span>def <span class="ident">user</span></span>(<span>self, *message: list[typing.Any])</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def user(self, *message: list[Any]):
    &#34;&#34;&#34;Send a user message. See `msg` for more details.

    Args:
        *message: A list of messages to be sent as system messages.

    Returns:
        LLMState: The llm instance itself, allowing for method chaining.
    &#34;&#34;&#34;
    return self.msg(&#34;user&#34;, *message)</code></pre>
</details>
<div class="desc"><p>Send a user message. See <code>msg</code> for more details.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>*message</code></strong></dt>
<dd>A list of messages to be sent as system messages.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="llmscope.LLMState" href="#llmscope.LLMState">LLMState</a></code></dt>
<dd>The llm instance itself, allowing for method chaining.</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul>
<li><a href="#llmscope">llmscope</a><ul>
<li><a href="#a-taste-of-llmscope">A Taste of llmscope</a></li>
<li><a href="#installation">Installation</a></li>
<li><a href="#usage">Usage</a><ul>
<li><a href="#2-key-llmstate-methods">2. Key LLMState Methods</a></li>
</ul>
</li>
<li><a href="#license">License</a></li>
</ul>
</li>
</ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="llmscope.fn" href="#llmscope.fn">fn</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="llmscope.LLMRequest" href="#llmscope.LLMRequest">LLMRequest</a></code></h4>
<ul class="two-column">
<li><code><a title="llmscope.LLMRequest.call_params" href="#llmscope.LLMRequest.call_params">call_params</a></code></li>
<li><code><a title="llmscope.LLMRequest.generate" href="#llmscope.LLMRequest.generate">generate</a></code></li>
<li><code><a title="llmscope.LLMRequest.messages" href="#llmscope.LLMRequest.messages">messages</a></code></li>
<li><code><a title="llmscope.LLMRequest.model" href="#llmscope.LLMRequest.model">model</a></code></li>
<li><code><a title="llmscope.LLMRequest.provider" href="#llmscope.LLMRequest.provider">provider</a></code></li>
<li><code><a title="llmscope.LLMRequest.response_model" href="#llmscope.LLMRequest.response_model">response_model</a></code></li>
<li><code><a title="llmscope.LLMRequest.tools" href="#llmscope.LLMRequest.tools">tools</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="llmscope.LLMState" href="#llmscope.LLMState">LLMState</a></code></h4>
<ul class="two-column">
<li><code><a title="llmscope.LLMState.assistant" href="#llmscope.LLMState.assistant">assistant</a></code></li>
<li><code><a title="llmscope.LLMState.call_params" href="#llmscope.LLMState.call_params">call_params</a></code></li>
<li><code><a title="llmscope.LLMState.config" href="#llmscope.LLMState.config">config</a></code></li>
<li><code><a title="llmscope.LLMState.generate" href="#llmscope.LLMState.generate">generate</a></code></li>
<li><code><a title="llmscope.LLMState.messages" href="#llmscope.LLMState.messages">messages</a></code></li>
<li><code><a title="llmscope.LLMState.model" href="#llmscope.LLMState.model">model</a></code></li>
<li><code><a title="llmscope.LLMState.msg" href="#llmscope.LLMState.msg">msg</a></code></li>
<li><code><a title="llmscope.LLMState.parse" href="#llmscope.LLMState.parse">parse</a></code></li>
<li><code><a title="llmscope.LLMState.provider" href="#llmscope.LLMState.provider">provider</a></code></li>
<li><code><a title="llmscope.LLMState.system" href="#llmscope.LLMState.system">system</a></code></li>
<li><code><a title="llmscope.LLMState.try_parse" href="#llmscope.LLMState.try_parse">try_parse</a></code></li>
<li><code><a title="llmscope.LLMState.try_tool" href="#llmscope.LLMState.try_tool">try_tool</a></code></li>
<li><code><a title="llmscope.LLMState.try_tools" href="#llmscope.LLMState.try_tools">try_tools</a></code></li>
<li><code><a title="llmscope.LLMState.user" href="#llmscope.LLMState.user">user</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
